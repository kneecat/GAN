from keras.datasets import mnist
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D
from keras.models import Sequential, Model
from keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import keras

class GAN():
  def __init__(self):
    #入力データのサイズ
    self.img_row = 28
    self.img_column = 28
    self.channel = 1
    self.img_shape = (self.img_row, self.img_column, self.channel)
    
    #<重要( ﾟДﾟ)>
    #潜在変数の次元
    self.z_dim = 100
    
    optimizer = Adam(0.0002, 0.5)
    
    #discriminator(検査役)
    self.discriminator = self.build_discriminator()
    self.discriminator.compile(loss="binary_crossentropy",
                               optimizer=optimizer,
                               metrics=["accuracy"])
    
    #generator(騙す役)
    self.generator = self.build_generator()
    self.combined = self.build_combined1()
    #self.combined = self.build_combined2()
    self.combined.compile(loss="binary_crossentropy", optimizer=optimizer)
    
  def build_generator(self):
    noise_shape = (self.z_dim,)
    model = Sequential()
    
    model.add(Dense(256, input_shape=noise_shape))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(1024))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(np.prod(self.img_shape), activation="tanh"))
    model.add(Reshape(self.img_shape))
    
    model.summary()
    return model
  
  def build_discriminator(self):
    model = Sequential()
    
    model.add(Flatten(input_shape=self.img_shape))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(256))
    model.add(LeakyReLU(alpha=0.2))
    #validを出力するため活性化関数はシグモイド関数！！
    model.add(Dense(1, activation="sigmoid"))
    model.summary()
    
    return model
  
  #学習させない検査役と騙す役をドッキングさせる方法　その１
  def build_combined1(self):
    self.discriminator.trainable = False
    model = Sequential([self.generator, self.discriminator])
    
    return model
  
  #　　　　　　　　       "　　　　　　　　　　　　　その２
  def build_combined2(self):
    z = Input(shape=(self.z_dim))
    img = self.generator(z)
    self.discriminator.trainable = False
    valid = self.discriminator(img)
    model = Model(z, valid)
    model.summary()
    
    return model
  
  def train(self, epochs, batch_size=128):
    (X_train, _), (_, _) = mnist.load_data()
    X_train = (X_train.astype(np.float32) - 127.5) / 127.5
    X_train = np.expand_dims(X_train, axis=3)
    
    harf_batch = int(batch_size / 2)
    num_batchs = int(X_train.shape[0] / harf_batch)
    print("Batch Size:", num_batchs)
    
    for epoch in range(epochs):
      #discriminator(検査役)の学習
      
      #バッチサイズの半分を偽物のデータにする
      noise = np.random.normal(0, 1, (harf_batch, self.z_dim))
      gen_imgs = self.generator.predict(noise)
      
      #残り半分を本物のデータにする
      idx = np.random.randint(0, X_train.shape[0], harf_batch)
      imgs = X_train[idx]
      
      d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((harf_batch, 1)))
      d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((harf_batch, 1)))
      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
      
      #generator(騙す役)の学習
      
      #バッチサイズの半分を偽物のデータにする
      noise = np.random.normal(0, 1, (batch_size, self.z_dim))
      valid_y = np.array([1] * batch_size)
      g_loss = self.combined.train_on_batch(noise, valid_y)
      
      print("%d [Discriminator loss: %f, acc.: %.2f%%] [Generator loss:%f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))
      if epoch == 0 or epoch == 1 or epoch == 100 or epoch == 200 or epoch == 300 or epoch == 400 or epoch == 500 or epoch == 1000 or epoch == 5000 or epoch == 10000 or epoch == 20000 or epoch == 30000:
        self.save_imgs(epoch)
      
  def save_imgs(self, epoch):
    r, c = 5,5
    noise = np.random.normal(0, 1, (r * c, self.z_dim))
    gen_imgs = self.generator.predict(noise)
    gen_imgs = 0.5 * gen_imgs + 0.5
        
    fig, axs = plt.subplots(r, c)
    cnt = 0
    for i in range(r):
      for j in range(c):
        axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap="gray")
        axs[i,j].axis("off")
        cnt += 1
            
    fig.savefig("fake_MNIST_%d.png" % epoch)
    plt.close()
    
if __name__ == "__main__":
  gan = GAN()
  gan.train(epochs=30001, batch_size=32)
